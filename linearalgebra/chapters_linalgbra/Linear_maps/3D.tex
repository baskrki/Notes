\subsection{Invertibility and Isomorphism}

\subsubsection{Invertible Linear Maps}

\begin{definition} 
    A linear map $T \in \LL(V,W)$ is called \textit{invertible} if there exists a linear map $S \in \LL(W,V)$ such that $ST$ equals identity
    operator on $V$ and $TS$ equals identity operator on $W$.
\end{definition}

\begin{definition}
    A linear map $S \in \LL(V,W)$ satisfying $ST=I$ and $TS=I$ is called an \textit{inverse} of $T$.
\end{definition}

\begin{proposition}
    An invertible map has an unique inverse.
\end{proposition}

\begin{proof}
    Suppose $T \in \LL(V,W)$ and let $S_1$ and $S_2$ be its inverses then
    \[ S_1 = S_1 I = S_1 (T S_2) = (S_1 T) S_2 = IS_2 = S_2 \]
\end{proof}

\begin{remark}
    Since inverses are unique for a invertible map $T$, we will denote it by $T^{-1}$. 
\end{remark}

\begin{proposition}
    A linear map is invertible if and only if it is injective and surjective.
\end{proposition}

\begin{proof}
    Suppose $T \in \LL(V,W)$ is an invertible map and suppose $T(v)=T(w)$ then
    \[ u = T^{-1}(Tu) = T^{-1}(Tv) = v\]
    Hence, $T$ is injective. To prove surjectivity, notice that
    \[  w = T^{-1}(Tw)\]
    which proves $T$ is surjective.

    Now, suppose $T$ is injective and surjective. Then, there exists a unique element $S(w)$ such that
    \[  T(S(w))=w\]
    the uniqueness is due to the injectivity of $T$. Let us show that, $S \in \LL(W,V)$
    \begin{align*}
        T(S(w_1)+S(w_2)) &= T(S(w_1)) + T(S(w_2)) \\
        &= w_1+w_2 \\ 
        &= T(S(w_1+w_2))
    \end{align*} 
    Thus, $S(w_1) + S(w_2) = S(w_1 + w_2)$. Also,
    \begin{align*}
        T(\lambda S(w)) &= \lambda T(S(w)) \\
        &= \lambda w \\
        &= T(S(\lambda w))
    \end{align*}
    Thus, $\lambda S(w)=S(\lambda w)$.
\end{proof}

Now, by how we defined $S$, it implies that $T S = I$ on $W$. Also,
\[ T(S T) v = (T S)(T) v = Tv  \]
\[ \implies (ST)v=v\]
Thus, $ST$ is an identity operator on $V$.

\begin{proposition}
    Suppose that $V$ and $W$ are finite-dimensional vector spaces, such that, $\dim W = \dim V$ and $T \in \LL(V,W)$. Then
    \[ T \text{ is invertible} \iff T \text{ is injective} \iff T \text{ is surjective} \]
\end{proposition}

\begin{proof}
From the Fundamental theorem of linear maps,
    \[ \dim V = \dim \operatorname{null} T + \dim \operatorname{range} T  \]
    If $T$ is injective then $\operatorname{null} T = \{0\}$. Thus
    \[ \dim V = \dim W = \dim \operatorname{range} T \]
    \[ \implies \operatorname{range} T = W \]
Now, if $T$ is surjective then $\operatorname{range} T = W$. Thus
\[ \dim V = \dim \operatorname{null} T  + \dim W\]
\[ \implies \dim \operatorname{range} T = 0 \]
\[ \implies \operatorname{range} T = \{0\} \]    
Thus, $T$ is injective $\iff$ $T$ is surjective. From \textbf{Proposition 1.13.} we get our final result.
\end{proof}


\begin{proposition}
    Suppose $V$ and $W$ are finite-dimensional vector spaces of the same dimension, $S \in \LL(V,W)$, and $T \in \LL(V,W)$. Then,
    $ST=I \iff TS=I$.
\end{proposition}

\begin{proof}
    First $ST=I$ then take $v \in \operatorname{null} T$. Thus,
    \[ v=STv = S(0)=0 \]
    Thus, $\operatorname{null} T = \{0\}$ and $T$ is injective. Since $\dim V = \dim W$ , this implies $T$ is invertible.
    Thus, there exists a $T^{-1}$. Now,
    \[ T^{-1}=(ST)(T^{-1}) =  S\] 
    We can now apply the same idea for $(\Leftarrow)$ of the proof. We just need to swap $V$ with $W$, and $T$ with $S$.
\end{proof}

\subsubsection{Isomorphic Vector Spaces}

\begin{definition}
    An \textit{isomorphism} is an invertible linear map and two vector spaces are isomorphic if there is an isomorphism between them.
\end{definition}

\begin{proposition}
    Two finite-dimensional vector spaces over $\mathbf{F}$ are isomorphic if and only if they have the same dimension.
\end{proposition}

\begin{proof}
    Suppose $V$ and $W$ are isomorphic. Then there exists a injective and surjective map $T$ from $V$ to $W$.
    Thus, $\operatorname{null} T = \{0\}$. Then
    \[ \dim V = \dim \operatorname{range} T  \]
    Also, since $T$ is surjective $\operatorname{range} T = W$. Then
    \[ \dim V = \dim W \]

    Now, suppose $\dim W = \dim V$. Define $T : V \to W$ as 
    \[ T(c_1 v_1 + \cdots + c_n v_n)=c_1 w_1 + \cdots + c_n w_n \]
    where $v_i$'s and $w_i$'s are the basis of $V$ and $W$ respectively. One can check this is a linear map. Now,
    this map is surjective as $\sum c_i w_i$ covers $W$. Also, $\operatorname{null} T = \{0\}$ as 
    \begin{align*}
        \dim W = \dim V &= \dim \operatorname{null} T + \dim \operatorname{range} T \\ 
        &= \dim \operatorname{null} T + \dim \operatorname{range} T \\
        &= \dim \operatorname{null} T + \dim W
    \end{align*}
    Thus, $T$ is injective and surjective which means that $V$ and $W$ are isomorphic.
\end{proof}

\begin{proposition}
    Suppose $v_1, \ldots, v_n$ be the basis of $V$ and $w_1, \ldots, w_m$ be the basis of $W$. Then $\MM(T)$ is a isomorphism between
    $\LL(V,W)$ to $\mathbf{F}^{m,n}$
\end{proposition}

\begin{proof}
    We know that $\MM(T)$ is a linear map as 
    \[ \MM(T+S) = \MM(T) + \MM(S) \quad \text{and} \quad  \MM(\lambda T ) = \lambda \MM(T) \] 
    
    We need to prove that $\MM$ is injective and surjective. We know that $\MM(T)$ is injective
    $ \iff \operatorname{null} \MM(T) = \{0\}$. And we know $\MM(T)=0 \iff T(x)=0$ for all $x \in V$. Thus, $T = 0$.

    To prove $\MM(T)$ is surjective. We know that there exists a $T \in \LL(V,W)$ such that
    \[ T(v_k) = \sum_{i=1}^{m} A_{i,j} w_j \]
    which proves the surjectivity of $\MM(T)$.
\end{proof}

\begin{proposition}
    Suppose $V$ and $W$ are finite-dimensional. Then $\LL(V,W)$ is finite-dimensional and
    \[ \dim \LL(V,W) = (\dim V ) (\dim W) \]
\end{proposition}

\begin{proof}
    Use \textbf{Proposition 1.17.} and \textbf{Proposition 1.16.} and 
    \[ \dim \LL(V,W) = mn = (\dim V)(\dim W) \]
\end{proof}

\subsubsection{Linear Map Thought of as Matrix Multiplication}

\begin{definition}
    Suppose $v \in V$ and $v_1, \ldots, v_n$ is the basis of $V$. The matrix of $v$ with respect to the basis is the $n \time 1$ matrix
    \[ \MM(v) = \begin{pmatrix}
        b_1 \\ 
        \vdots \\
        b_n
    \end{pmatrix} \]
    where $b_1, \ldots, b_n$ are scalar such that $v = b_1 v_1 + \cdots + b_n v_n$.
\end{definition}

\begin{proposition}
    Suppose $T \in \LL(V,W)$ and $v_1, \ldots, v_n$ is a basis of $V$ and $w_1, \ldots, w_n$ is a basis of $W$. Let $1 \le k \le n$.
    Then,
    \[ \MM(T)_{\cdot, k} = \MM(Tv_k) \]
\end{proposition}

\begin{proof}
    Immediate from the definition of $\MM(Tv_k)$.
\end{proof}

\begin{proposition}
    Suppose $T \in \LL(V,W)$ and $v \in V$. Let $v_1, \ldots, v_n$ be the basis of $V$ and $w_1, \ldots, w_m$ be the basis of $W$. Then
    \[ \MM(Tv) = \MM(T) \MM(v) \] 
\end{proposition}

\begin{proof}
    Suppose $v=b_1v_1 + \cdots + b_n v_n$. Then,
    \[ Tv = b_1 Tv_1 + \cdots + b_n Tv_n \]
    Hence, 
    \begin{align*}
        \MM(Tv) &= b_1 \MM(Tv_1) + \cdots + b_n \MM(Tv_n)  & \textit{(Linearity of } \MM \textit{)} \\
        &= b_1 \MM(T)_{\cdot,1} + \cdots + b_n \MM(T)_{\cdot, n} & \textit{(Proposition 1.19.1)} \\
        &= \MM(T) \MM(v) & \textit{(Theorem 1.8.)}
    \end{align*}
\end{proof}

\begin{proposition}
    Suppose $V$ and $W$ are finite-dimensional and $T \in \LL(V,W)$. Then $\dim \operatorname{range} T$ equals the column rank of 
    $\MM(T)$.
\end{proposition}

\begin{proof}
    Suppose $v_1, \ldots, v_n$ be the basis of $V$ and $w_1, \ldots, w_m$ be the basis of $W$. Now, define 
    $\varphi : W \to \mathbf{F}^{m,1}$ as $\varphi(w)=\MM(w)$. One can prove that this is an isomorphism. If we restrict our domain to just
    $\operatorname{range} T$ we see that our co-domain is going to be $\mathcal{O}=\operatorname{span}\{\MM(Tv_1),\ldots,\MM(Tv_m)\}$. Also,
    \[ \varphi \mid_{\operatorname{range} T} : \operatorname{range} T \to \mathcal{O} \] 
    is a isomorphism and since isomorphism preserves dimension. We have
    \[ \dim \operatorname{range} T = \dim \mathcal{O} = \text{column rank of } T \]
\end{proof}

\subsubsection{Change of Basis}

\begin{definition}
    We define the $n \times n$ matrix, called \textit{identity matrix}
    by
    \[ A_{i,j} = \begin{cases}
        1 & i = j \\
        0 & i \neq j
    \end{cases} \]
    The identity matrix is denoted by $I$.
\end{definition}

\begin{definition}
    A square matrix is called \textit{invertible} if there is a square matrix $B$ of the same size such that
    \[ AB = BA = I \]
    we call the matrix $B$ the \textit{inverse} of $A$.
\end{definition}

\begin{remark}
    The inverse of a square matrix $A$ is unique and therefore will be denoted by $A^{-1}$. Here, is a short proof of the uniqueness of the 
    inverse. Suppose $A$ has two inverses $B_1 $ and $B_2$. Thus, 
    \[ B_1 = I B_1 = (B_2A)B_1 = B_2 (AB_1) = B_2I = B_2  \]
    Also, $(A^{-1})^{-1}=I$ and $(AC)^{-1}=C^{-1}A^{-1}$. You can verify these.
\end{remark}

\begin{definition}
    Suppose $T \in \LL(U,V)$ and $S \in \LL(V,W)$. If $u_1, \ldots, u_m$ is a basis of $U$, $v_1, \ldots, v_n$ is a basis of $V$, and 
    $w_1, \ldots, w_p$ is the basis of $W$ then
    \begin{align*}
        \MM(ST,(u_1, \ldots, u_m),(w_1, \ldots, w_p)) = \MM(S,(v_1,\ldots,v_n),&(w_1, \ldots, w_p)) \\
        &\MM(T,(u_1,\ldots,u_m),(v_1, \ldots, v_n) ) 
    \end{align*}
\end{definition}

This is just the matrix multiplication which we had defined earlier but with respect to the basis. See \textbf{Proposition 1.11.}


\begin{proposition}
    Suppose $u_1, \ldots, u_n$ and $v_1, \ldots, v_n$ are the basis of $V$. Then the matrices 
    \[ \MM(I,(u_1,\ldots,u_n),(v_1,\ldots,v_n)) \quad \text{and} \quad \MM(I,(v_1,\ldots,v_n),(u_1,\ldots,u_n)) \]
    are inverses of each other. Here, $I$ is the identity operator.
\end{proposition}

\begin{proof}
    Use \textbf{Definition 1.23.} and replace $w_k$ with $u_k$. And replace $S,T$ with the identity operator. Then
    \[ I = \MM(I,(v_1,\ldots,v_n),(u_1,\ldots,u_n)) \MM(I,(u_1,\ldots,u_n),(v_1,\ldots,v_n)) \]
    Now interchange the roles of $u$'s and $v$'s to get
    \[ I = \MM(I,(u_1,\ldots,u_n),(v_1,\ldots,v_n)) \MM(I,(v_1,\ldots,v_n),(u_1,\ldots,u_n)) \]
\end{proof}

\begin{remark}
    For convenience, we'll write
    \[ \MM(T,(u_1,\ldots,u_n),(u_1,\ldots,u_n)) = \MM(T,(u_1,\ldots,u_n)) \] 
\end{remark}

\begin{proposition}
    Suppose $T \in \LL(V)$. Let $u_1, \ldots, u_n$ and $v_1, \ldots, v_n$ be the basis of $V$. Let 
    \[ A = \MM(T,(u_1,\ldots,u_n)) \quad \text{and} \quad B = \MM(T,(v_1, \ldots, v_n)) \]
    and $C=\MM(I,(u_1,\ldots,u_n),(v_1,\ldots,v_n))$. Then,
    \[ A = C^{-1} BC \]
\end{proposition}

\begin{proof}
    Use \textbf{Definition 1.23.} and replace $w_k$ with $u_k$ and $S$ with $I$. Then, use \newline \textbf{Proposition 1.22.} to get
    \begin{equation}
        A = C^{-1} \MM(T,(u_1,\ldots,u_n),(v_1,\ldots,v_n))
    \end{equation}
    Now, again use the definition and this time replace $w_k$ with $v_k$. Then
    \[ \MM(T,(u_1,\ldots,u_n),(v_1,\ldots,v_n)) = BC \]
    We can now substitute this equation in equation $(1)$ to get 
    \[ A = C^{-1}BC \]
\end{proof}


\begin{proposition}
    Suppose that $v_1,\ldots,v_n$ is the basis of $V$ and $T \in \LL(V)$ is invertible. Then, 
    $\MM(T^{-1})=(\MM(T))^{-1}$, where both matrices are with respect to basis $v_1,\ldots,v_n$.
\end{proposition}


\begin{proof}
    Use \textbf{Definition 1.23.}
\end{proof}

\subsubsection{Exercise}


\paragraph{Problem :} Suppose $T \in \LL(V,W)$ is invertible. Show that $T^{-1}$ is invertible and 
\[ (T^{-1})^{-1}  = T \]

\vspace{4mm}
\textit{Solution :} Since, $T$ is invertible, we have 
\[ T T^{-1} = I \quad \text{and} \quad T^{-1} T = I \]
If we switch our perspective from $T$ to $T^{-1}$, we get that $T$ is invertible from \textbf{Definition 1.17.} and from 
\textbf{Proposition 1.12.} we have 
\[ (T^{-1})^{-1} = T \]

\paragraph{Problem :} Suppose $T \in \LL(U,V)$ and $S \in \LL(V,W)$ are both invertible linear maps. Prove that $ST \in \LL(U,W)$ is 
invertible and that $(ST)^{-1} = T^{-1} S^{-1}$.

\vspace{4mm}
\textit{Solution :} Since, $S$ and $T$ are both invertible then $S^{-1}$ and $T^{-1}$ both exist. Also, $T^{-1} S^{-1} \in \LL(W,U)$.
Thus,
\begin{align*}
    (ST)(T^{-1} S^{-1}) &= S(TT^{-1})S^{-1} \\
    &= S(I)S^{-1} \\
    &= SS^{-1} \\
    &= I
\end{align*}
One can do the same thing for $(T^{-1}S^{-1})(ST)$. Thus, $ST$ is invertible and from the above calculation so 
we can say $(ST)^{-1}=T^{-1}S^{-1}$.

\paragraph{Problem :} Suppose $V$ is finite-dimensional and $V \in \LL(V)$. Prove that the following are equivalent.
\begin{enumerate}
    \item[(a)] $T$ is invertible
    \item[(b)] $Tv_1,\ldots,Tv_n$ is a basis of $V$ for every basis $v_1,\ldots,v_n$ of $V$.
    \item[(c)] $Tv_1,\ldots,Tv_n$ is a basis of $V$ for some basis $v_1, \ldots,v_n$ of $V$.
\end{enumerate}

\vspace{4mm}
\textit{Solution :} Suppose $T$ is invertible then it is also injective and surjective. Let $v_1, \ldots, v_n$ be a basis of $V$.
Then, we know that $\operatorname{span}\{Tv_1,\ldots,Tv_n\} = V$ because of the surjectivity. Also, if 
\begin{align*}
    a_1 Tv_1 + \cdots + a_n Tv_n &= 0 \\
    \implies T(a_1v_1 + \cdots + a_n v_n) &= 0 \\
    \implies a_1 v_1 + \cdots + a_n v_n &= 0 \\
    \implies a_1=a_2=\cdots=a_n&=0
\end{align*}
The last line is from injectivity of $T$. Thus, $Tv_1, \ldots, Tv_n$ is a basis of $V$ for any basis of $V$.

Now, suppose $Tv_1,\ldots,Tv_n$ is a basis of $V$ for every basis $v_1,\ldots,v_n$ of $V$. Then, (c) automatically holds. Also,
\[ a_1 Tv_1 + \ldots + a_n Tv_n = 0 \]
\[ \implies a_1 = a_2 = \cdots = a_n = 0 \]
Thus, $\operatorname{null} T = \{0\}$ which implies $T$ is injective. Now, since $Tv_1,\ldots,Tv_n$ is a basis, every element of $V$ can 
be written as some combination of $V$. Thus,
\[ a_1 Tv_1 + \cdots + a_n Tv_n = y \]
\[ \implies T(a_1 v_1 + \cdots + a_n v_n) = y \]
Thus, for every $y \in V$ there exists some element which gets mapped to $y$. Thus, $T$ is surjective. Thus, from \textbf{Proposition 1.13.}
we get that $T$ is invertible.

Now, suppose $Tv_1,\ldots,Tv_n$ is a basis of $V$ for some basis $v_1, \ldots,v_n$ of $V$. Then we can apply the same argument as we did 
for above to get to $T$ is invertible. Since, $T$ is invertible we get (b).

\paragraph{Problem :} Suppose $V$ is finite-dimensional and $\dim V > 1$. Prove that the set of non-invertible linear maps from $V$ to 
itself is not a subspace of $\LL(V)$.

\vspace{4mm}
\textit{Solution :} We can construct two non-invertible linear maps which form a invertible map when added.
Suppose $v_1, \ldots, v_n$ is a basis of $V$. 
\[ T(a_1 v_1 + \cdots + a_n v_n ) = a_2 v_2 + \cdots + a_n v_n \]
\[ S(a_1 v_1 + \cdots + a_n v_n ) = a_1 v_1 \]
One can check that both of them are linear maps and both of them lack injectivity property so they're not invertible.
But 
\[ (S+T)(a_1 v_1 + \cdots + a_n v_n ) = a_1 v_1 + a_2 v_2 + \cdots + a_n v_n \]
\[ (S+T)(x)=I(x) \]
which is a invertible linear map. Thus, set of non-invertible linear maps from $V$ to itself is not a subspace of $\LL(V)$.

\begin{remark}
    We used the $\dim V > 1$ when we defined $T$ and $S$.
\end{remark}

\paragraph{Problem :} Suppose $V$ is finite-dimensional, $U$ is a subspace of $V$, and $S \in \LL(U,V)$. Prove that there exists a invertible
linear map $T$ from $V$ to itself such that $Tu=Su$ for every $u \in U$ if and only if $S$ is injective.

\vspace{4mm}
\textit{Solution :} For $(\Rightarrow)$, if $S(x)=S(y)$ then $T(x)=T(y)$ which implies $x=y$ because $T$ is invertible. 
Now, for $(\Leftarrow)$ choose a basis of $U$ and extend it to the basis of $V$ say $\mathcal{B}=(u_1,\ldots,u_k,v_{k+1},\ldots,v_n)$, 
here $n = \dim V$. Now since $S$ is injective, the list $(Su_1 , \ldots, S u_k)$ is linearly independent and can be extended to a basis of $V$.
Let 
\[ \mathcal{C}=\{Su_1,\ldots,Su_k,w_{k+1},\ldots,w_n\} \] 
be the basis of $V$. Define $T : V \to V$ as following 
\[ T(u_i) = S(u_i) \text{ for } 1 \le i \le k \quad \text{and} \quad T(v_j)=w_j  \text{ for } k+1 \le j \le n \]

One can check this is a invertible linear map.

\paragraph{Problem :} Suppose $W$ is finite-dimensional and $S,T \in \LL(V,W)$. Prove that $\operatorname{null} S = \operatorname{null} T$
if and only if there exists an invertible $E \in \LL(W)$ such that $S = E T$.

\vspace{4mm}
\textit{Solution :} For $(\Leftarrow)$, take $x \in \operatorname{null} T$ then $S(x) = E(T(x))= 0$ thus $x \in \operatorname{null} S$.
Now, if $x \in \operatorname{null} S$ then $0=S(x)=ET(x) \implies T(x)=0$ thus $x \in \operatorname{null} T$. Thus,
$\operatorname{null} T = \operatorname{null} S$. I'll do the $\Leftarrow$ later.